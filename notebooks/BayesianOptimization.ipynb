{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da88d3f",
   "metadata": {},
   "source": [
    "# <span style=\"color:#3CB371;\">Mastering Bayesian Optimization: From Theory to Hyperparameter Tuning</span>\n",
    "\n",
    "<!-- <span style=\"color:#A9A9A9;\"> -->\n",
    "Bayesian Optimization (BO) is a powerful method for optimizing expensive, black-box, or noisy objective functions. Unlike grid search or random search, BO intelligently chooses the next sampling point using a probabilistic model, reducing the number of evaluations needed.\n",
    "\n",
    "- This notebook covers:\n",
    "- 1. Bayesian Optimization Theory\n",
    "- 2. Pros and Cons\n",
    "- 3. Applications\n",
    "- 4. From-scratch Python Implementation\n",
    "- 5. XGBoost Hyperparameter Tuning with BO\n",
    "- 6. Comparison with Random and Grid Search\n",
    "- 7. Limitations\n",
    "<!-- </span> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f358ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30183577",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">1. What is Bayesian Optimization?\n",
    "\n",
    "Bayesian Optimization is a strategy to find the **minimum or maximum of an expensive-to-evaluate function** `f(x)` efficiently. It is particularly useful when:\n",
    "\n",
    "- Evaluations are costly (time or computation)\n",
    "- Function is noisy or unknown (black-box)\n",
    "- Gradient information is unavailable\n",
    "\n",
    "BO uses two main components:\n",
    "\n",
    "1. **Surrogate Model**: A probabilistic model (usually a Gaussian Process) approximates `f(x)`.\n",
    "2. **Acquisition Function**: Determines the next sampling point using the surrogate model to balance **exploration vs. exploitation**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#008B8B;\">Mathematical Concepts\n",
    "\n",
    "\n",
    "#### Gaussian Process (GP) Surrogate\n",
    "A GP defines a distribution over functions:\n",
    "f(x) ~ GP(m(x), k(x, x'))\n",
    "\n",
    "Where:\n",
    "- m(x) = mean function (often 0)\n",
    "- k(x, x') = covariance/kernel function, e.g., squared exponential:\n",
    "k(x, x') = exp(-||x-x'||^2 / (2*l^2))\n",
    "\n",
    "GP gives:\n",
    "- Mean mu(x) -> predicted function value\n",
    "- Variance sigma^2(x) -> uncertainty\n",
    "\n",
    "\n",
    "#### Acquisition Functions\n",
    "These guide **where to sample next**.\n",
    "\n",
    "\n",
    "1. **Expected Improvement (EI)**:\n",
    "EI(x) = E[max(f(x) - f(x^+), 0)]\n",
    "Where f(x^+) is the current best observed value.\n",
    "\n",
    "\n",
    "2. **Probability of Improvement (PI)**:\n",
    "PI(x) = P(f(x) > f(x^+) + xi)\n",
    "\n",
    "\n",
    "3. **Upper Confidence Bound (UCB)**:\n",
    "UCB(x) = mu(x) + k * sigma(x)\n",
    "\n",
    "- k balances exploration (large sigma) vs. exploitation (high mu).\n",
    "\n",
    "### <span style=\"color:#008B8B;\">Bayesian Optimization Algorithm\n",
    "\n",
    "\n",
    "1. Initialize: Sample f(x) at a few points.\n",
    "2. Build GP Surrogate: Fit GP to initial data.\n",
    "3. Select Next Point: Maximize acquisition function.\n",
    "4. Evaluate Objective: Compute f(x_next).\n",
    "5. Update GP: Include new data point.\n",
    "6. Repeat until convergence criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42973e8f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">2. Pros and Cons\n",
    "\n",
    "\n",
    "**Pros:**\n",
    "- Efficient: fewer evaluations than grid/random search\n",
    "- Handles noisy/black-box functions\n",
    "- Provides uncertainty quantification\n",
    "- Can optimize non-convex, discontinuous functions\n",
    "\n",
    "\n",
    "**Cons:**\n",
    "- High computational cost for GP in high dimensions\n",
    "- Sensitive to choice of surrogate model and acquisition function\n",
    "- Struggles in very high-dimensional spaces\n",
    "- Sequential nature (hard to parallelize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e7dde",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">3. Applications\n",
    "\n",
    "- Hyperparameter tuning (ML models like XGBoost, deep learning)\n",
    "- Robotics (optimizing gait or control policies)\n",
    "- Chemical engineering (experiment optimization)\n",
    "- A/B testing (marketing/product experiments)\n",
    "- Simulations (scientific experiments with expensive evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf02b9",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">4. Bayesian Optimization from Scratch in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b352d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x_next=1.9069, f(x_next)=3.0087\n",
      "Iteration 2: x_next=2.0120, f(x_next)=3.0001\n",
      "Iteration 3: x_next=3.6737, f(x_next)=5.8012\n",
      "Iteration 4: x_next=0.0000, f(x_next)=7.0000\n",
      "Iteration 5: x_next=0.0000, f(x_next)=7.0000\n",
      "Iteration 6: x_next=0.0000, f(x_next)=7.0000\n",
      "Iteration 7: x_next=0.0000, f(x_next)=7.0000\n",
      "Iteration 8: x_next=0.0000, f(x_next)=7.0000\n",
      "Iteration 9: x_next=0.0000, f(x_next)=7.0000\n",
      "Iteration 10: x_next=0.0000, f(x_next)=7.0000\n",
      "Best x: 2.0120, f(x)=3.0001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# --- Objective Function (Black-box) ---\n",
    "def f(x):\n",
    "    return (x - 2)**2 + 3\n",
    "\n",
    "# --- Gaussian Process Kernel ---\n",
    "def rbf_kernel(x1, x2, length_scale=1.0, sigma_f=1.0):\n",
    "    sqdist = np.subtract.outer(x1, x2)**2\n",
    "    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n",
    "\n",
    "# --- Gaussian Process Prediction ---\n",
    "def gp_predict(X_train, Y_train, X_s, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "    K = rbf_kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "    K_s = rbf_kernel(X_train, X_s, l, sigma_f)\n",
    "    K_ss = rbf_kernel(X_s, X_s, l, sigma_f) + 1e-6 * np.eye(len(X_s))\n",
    "    K_inv = np.linalg.pinv(K)\n",
    "    mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "    return mu_s.ravel(), np.diag(cov_s)\n",
    "\n",
    "# --- Expected Improvement Acquisition ---\n",
    "def expected_improvement(X_s, X_train, Y_train, xi=0.01):\n",
    "    mu, sigma = gp_predict(X_train, Y_train, X_s)\n",
    "    mu_sample_opt = np.min(Y_train)\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu_sample_opt - mu - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma==0.0] = 0.0\n",
    "    return ei\n",
    "\n",
    "# --- Bayesian Optimization Loop ---\n",
    "X_train = np.array([0.0, 2.5, 5.0])  # # initial samples\n",
    "Y_train = f(X_train)\n",
    "\n",
    "\n",
    "bounds = (0, 5)\n",
    "n_iter = 10\n",
    "\n",
    "\n",
    "for i in range(n_iter):\n",
    "    X_s = np.linspace(bounds[0], bounds[1], 1000)\n",
    "    ei = expected_improvement(X_s, X_train, Y_train)\n",
    "    x_next = X_s[np.argmax(ei)]\n",
    "    y_next = f(x_next)\n",
    "    X_train = np.append(X_train, x_next)\n",
    "    Y_train = np.append(Y_train, y_next)\n",
    "    print(f\"Iteration {i+1}: x_next={x_next:.4f}, f(x_next)={y_next:.4f}\")\n",
    "\n",
    "\n",
    "best_idx = np.argmin(Y_train)\n",
    "print(f\"Best x: {X_train[best_idx]:.4f}, f(x)={Y_train[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11cf32",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">5. XGBoost Hyperparameter Tuning with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea64e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.4674\n",
      "Best parameters:\n",
      "  max_depth: 9\n",
      "  learning_rate: 0.0631960890611875\n",
      "  n_estimators: 401\n",
      "  subsample: 0.7984250789732436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# Load Data\n",
    "data = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "space  = [\n",
    "    Integer(3, 10, name='max_depth'),\n",
    "    Real(0.01, 0.3, name='learning_rate'),\n",
    "    Integer(50, 500, name='n_estimators'),\n",
    "    Real(0.5, 1.0, name='subsample'),\n",
    "]\n",
    "\n",
    "# Objective function\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model = xgb.XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        **params\n",
    "    )\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_root_mean_squared_error')\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "res_gp = gp_minimize(objective, space, n_calls=20, random_state=42)\n",
    "\n",
    "print(\"Best score: %.4f\" % res_gp.fun)\n",
    "print(\"Best parameters:\")\n",
    "for name, val in zip([s.name for s in space], res_gp.x):\n",
    "    print(f\"  {name}: {val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592532d9",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">6. Advantages over Random/Grid Search\n",
    "\n",
    "\n",
    "| Feature | Grid Search | Random Search | Bayesian Optimization |\n",
    "|---------------------------|------------|---------------|---------------------|\n",
    "| Evaluations Required | High | Medium | Low |\n",
    "| Handles Expensive Func | ❌ | ❌ | ✅ |\n",
    "| Exploits Past Knowledge | ❌ | ❌ | ✅ |\n",
    "| Can Optimize Noisy Funcs | ❌ | ❌ | ✅ |\n",
    "\n",
    "\n",
    "BO learns from previous trials, focusing on promising regions—unlike random or grid search which wastes evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a35486",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">7. Limitations\n",
    "\n",
    "\n",
    "- **Scalability**: GP surrogate scales poorly in high dimensions\n",
    "- **Computational Overhead**: Inverting kernel matrices is expensive\n",
    "- **Sequential**: Hard to parallelize without modifications\n",
    "- **Hyperparameter Sensitivity**: Choice of kernel and acquisition function affects performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc726a8",
   "metadata": {},
   "source": [
    "## <span style=\"color:#20B2AA;\">✅ Conclusion\n",
    "\n",
    "\n",
    "Bayesian Optimization provides an **efficient, probabilistic framework** for optimizing black-box functions and tuning ML hyperparameters. While it has limitations in high dimensions, for expensive or noisy functions, BO often outperforms random and grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
