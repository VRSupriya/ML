{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4cd48a",
   "metadata": {},
   "source": [
    "# ðŸš€ New Advance in Parameter-Efficient Fine-Tuning: LoRA-XS\n",
    "\n",
    "Fine-tuning large language models (LLMs) usually requires millions of trainable parameters per task â€“ making personalization and multi-task deployment very costly.  \n",
    "\n",
    "ðŸ”‘ **Enter LoRA-XS** (Low-Rank Adaptation with eXtremely Small parameters).  \n",
    "This method drastically reduces fine-tuning overhead while matching or outperforming existing approaches like LoRA and VeRA.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Key Highlights\n",
    "\n",
    "<img src=\"images/lora-xs.png\" alt=\"LoRA-XS Diagram\" width=\"400\">\n",
    "\n",
    "- âœ… **Tiny adapters** â†’ freeze the top singular vectors (from SVD of pre-trained weights) and train only a small *rÃ—r* matrix.  \n",
    "- âœ… **Drastic savings** â†’ only *rÂ²* parameters per module, independent of model size.  \n",
    "- âœ… **Scalable personalization** â†’ adapting GPT-3 for 1M users requires **96 GB with LoRA-XS** vs. **144 TB with LoRA**.  \n",
    "- âœ… **Strong performance** â†’ on GLUE, GSM8K, MATH, and commonsense reasoning benchmarks, LoRA-XS matches or beats LoRA/VeRA despite being 100Ã— smaller.  \n",
    "- âœ… **Flexible** â†’ works from as little as 1 parameter per module up to larger sizes, depending on memory constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Why It Matters\n",
    "\n",
    "By aligning fine-tuning with the top singular vectors of pre-trained weights, LoRA-XS enables ultra-efficient adaptation. This makes personalized, task-specific, and multi-tenant deployments of LLMs far more practical.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“„ Full Paper\n",
    "\n",
    "[LoRA-XS: Low-Rank Adaptation with eXtremely Small Number of Parameters](https://lnkd.in/gMwSk2Nu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b87c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4391040f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
